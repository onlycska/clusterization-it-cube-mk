{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad4ae9bc-5e10-4a9b-848f-daaec380d86e",
   "metadata": {},
   "source": [
    "# Задача разбора мусорки*\n",
    "*Когда все документы берутся как документы из одной кучи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f708d-2b8e-453b-a16b-ae2ec8d879f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from dataset import DatasetTextExtractor\n",
    "from dataset import add_pred_target_name\n",
    "from vectorizers import Tfidf, CountVec\n",
    "from sklearn.decomposition import PCA\n",
    "from vizualization import plot_cluster, plot_confusion_matrix\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918c54f-b911-42ba-96e9-36905d8dd631",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0c800-7079-4c19-a509-17c60f2a8d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим и сохраним датасет\n",
    "from pathlib import Path\n",
    "\n",
    "root_path = Path('./')\n",
    "dataset_raw_path = root_path.joinpath('dataset')\n",
    "dataset_save_path = root_path.joinpath('saved_dataset.pickle')\n",
    "\n",
    "dl = DatasetTextExtractor()\n",
    "if dataset_save_path.is_file():\n",
    "    dataset = dl.load(dataset_save_path)\n",
    "else:\n",
    "    dataset = dl.read(dataset_raw_path)\n",
    "    dl.save(dataset_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fad0a-6fa9-40fe-b1d8-00c261044402",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b3b25-b6f7-469c-9928-1a02ee1e91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_path = root_path.joinpath('vectors')\n",
    "vectors_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# tfidf\n",
    "tf_idf_vectors_save_path = vectors_path.joinpath('tfidf.pickle')\n",
    "\n",
    "tfidf_vectorizer = Tfidf()\n",
    "if tf_idf_vectors_save_path.is_file():\n",
    "    tfidf_vectors = tfidf_vectorizer.load(tf_idf_vectors_save_path)\n",
    "else:\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(dataset['text'].values)\n",
    "    tfidf_vectorizer.save(tf_idf_vectors_save_path)\n",
    "\n",
    "# count\n",
    "count_vectors_save_path = vectors_path.joinpath('count.pickle')\n",
    "\n",
    "count_vectorizer = CountVec()\n",
    "if count_vectors_save_path.is_file():\n",
    "    count_vectors = count_vectorizer.load(count_vectors_save_path)\n",
    "else:\n",
    "    count_vectors = count_vectorizer.fit_transform(dataset['text'].values)\n",
    "    count_vectorizer.save(count_vectors_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d912b-a693-4016-88f2-7b5104abb575",
   "metadata": {},
   "source": [
    "## Кластеризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd24ab-5863-4d27-8def-b4e644007b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, SpectralClustering, DBSCAN\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4345662-ea4f-46b1-bfa5-783e3b009dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterization(estimator, X, dataset, key):\n",
    "    \"\"\"\n",
    "    estimator - кластеризатор\n",
    "    X - вектора\n",
    "    dataset - исходный датасет который обновляем полученными данными\n",
    "    key - уникальная строка для метода веторизации, кластеризатор, по ней будут формировать в dataset названия столбцов\n",
    "    \n",
    "    \n",
    "    Обучить кластеризатор\n",
    "    Вывести метрики\n",
    "    Добавить в исходный датасет предсказанные кластера и названия кластеров\n",
    "    Визуализировать кластер\n",
    "    Вывести \n",
    "    \"\"\"\n",
    "    \n",
    "    # train\n",
    "    estimator.fit(X)\n",
    "    labels = estimator.labels_\n",
    "    \n",
    "    # metrics     \n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    print(estimator)\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    print('Estimated number of noise points: %d' % n_noise_)\n",
    "    \n",
    "    if n_clusters_:\n",
    "        print(\"Silhouette Coefficient on X: %0.3f\" % metrics.silhouette_score(X, labels))\n",
    "\n",
    "        predicted_target_column_name = f'{key}_target'\n",
    "        predicted_target_name_column_name = f'{predicted_target_column_name}_name'\n",
    "        \n",
    "        dataset[predicted_target_column_name] = labels\n",
    "\n",
    "\n",
    "        # reduce data\n",
    "        pca_2D = PCA(n_components=2, random_state=random_state)\n",
    "        reduced_pca_2D = pca_2D.fit_transform(X.toarray())\n",
    "        \n",
    "        print(\"Silhouette Coefficient on reduced 2D tfidf: %0.3f\" % metrics.silhouette_score(reduced_pca_2D, labels))\n",
    "        \n",
    "        # cluster visualization with cluster ID\n",
    "        plot_cluster(features=reduced_pca_2D, y_pred=labels, y_labels=dataset[['file_name']])\n",
    "    \n",
    "    \n",
    "        add_pred_target_name(dataset, predicted_target_column_name, 'target_name', predicted_target_name_column_name)\n",
    "\n",
    "        # plot confusion matrix \n",
    "        y_true = dataset.target_name.values\n",
    "        y_pred = dataset[predicted_target_name_column_name].values\n",
    "        cm_labels = dataset.target_name.unique()\n",
    "\n",
    "        plot_confusion_matrix(y_true, y_pred, cm_labels)\n",
    "\n",
    "        # cluster visualization with kind names\n",
    "        plot_cluster(features=reduced_pca_2D, y_pred=y_true, y_labels=dataset[['file_name']])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация разными алгоритмами\n",
    "## Первый график - распределение кластеров в кластеризации\n",
    "## Второй график - эталонное распределение данных по кластерам\n",
    "## Таблица - пересечение предсказанных лейблов и эталонных.\n",
    "Задача: \n",
    "Сейчас минимальное количество ошибок в текущем состоянии - 52. Попробуй понизить их число в 2 раза."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe517c-6c49-4536-a3df-2015d61bd7b1",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d12b25a-ed03-4477-8e7d-deda8b2a13eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "\n",
    "estimator = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusterization(estimator, tfidf_vectors, dataset, 'kmeans_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94b69bc-6d29-4d76-ba6f-fc34fa25686a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "tfidf_vectors_normalized = preprocessing.normalize(tfidf_vectors, norm='l2')\n",
    "clusterization(estimator, tfidf_vectors_normalized, dataset, 'kmeans_tfidf_normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b05f5-40e1-4c87-8640-1e68078661bd",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc0ffa-f789-4164-9fa5-ecaf5e9cd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DBSCAN(eps=0.3, min_samples=10)\n",
    "clusterization(estimator, tfidf_vectors, dataset, 'dbscan_tfidf_03_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc708748-30e5-4b2c-9b05-7949faf0a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DBSCAN(eps=0.3, min_samples=10, metric='cosine')\n",
    "clusterization(estimator, tfidf_vectors, dataset, 'dbscan_tfidf_03_10_cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e534b-ff07-49c0-b7c4-09c5b4f9add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DBSCAN(eps=0.3, min_samples=10, metric='cosine', algorithm='brute')\n",
    "clusterization(estimator, tfidf_vectors, dataset, 'dbscan_tfidf_03_10_cosine_brute')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c55d5-9f89-48f6-b752-f8da3867a8f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = DBSCAN(eps=0.9, min_samples=500, metric='cosine', algorithm='brute')\n",
    "clusterization(estimator, tfidf_vectors, dataset, 'dbscan_tfidf_01_100_cosine_brute')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc65317-6fde-4688-acc2-535d776f1eb2",
   "metadata": {},
   "source": [
    "## hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c72d0-2f1a-4f4b-a28a-21bb9eca8255",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "estimator = hdbscan.HDBSCAN()\n",
    "clusterization(estimator, tfidf_vectors, dataset, 'hdbscan')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce18ce38fd77408b5742955dabcc9642b804b82e43ed8f64b18b318a61447a9f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}